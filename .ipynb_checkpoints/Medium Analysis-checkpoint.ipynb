{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As the data file included the cover photo, resulting in a 1GB file size. I downloaded the file, unzipped to the working directory named 'medium_data.csv'. The images will be used for another image analysis.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Coding\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6508.000000</td>\n",
       "      <td>6508.000000</td>\n",
       "      <td>6508.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3254.500000</td>\n",
       "      <td>311.076060</td>\n",
       "      <td>6.134911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1878.842108</td>\n",
       "      <td>950.789896</td>\n",
       "      <td>3.231918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1627.750000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3254.500000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4881.250000</td>\n",
       "      <td>268.250000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6508.000000</td>\n",
       "      <td>38000.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id         claps  reading_time\n",
       "count  6508.000000   6508.000000   6508.000000\n",
       "mean   3254.500000    311.076060      6.134911\n",
       "std    1878.842108    950.789896      3.231918\n",
       "min       1.000000      0.000000      0.000000\n",
       "25%    1627.750000     54.000000      4.000000\n",
       "50%    3254.500000    115.000000      5.000000\n",
       "75%    4881.250000    268.250000      7.000000\n",
       "max    6508.000000  38000.000000     55.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('medium_data.csv')\n",
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3  https://towardsdatascience.com/databricks-how-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "\n",
       "                                  subtitle   image  claps responses  \\\n",
       "0                                      NaN   1.png    850         8   \n",
       "1                                      NaN   2.png   1100        11   \n",
       "2         A Grammar of Graphics for Python   3.png    767         1   \n",
       "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             8  Towards Data Science  2019-05-30  \n",
       "1             9  Towards Data Science  2019-05-30  \n",
       "2             5  Towards Data Science  2019-05-30  \n",
       "3             4  Towards Data Science  2019-05-30  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = dat.iloc[:,1:]\n",
    "dat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.drop(index = dat[dat['responses'] == 'Read'].index)\n",
    "dat['reading_time'] = dat['reading_time'].astype('float')\n",
    "dat['responses'] = dat['responses'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url              object\n",
       "title            object\n",
       "subtitle         object\n",
       "image            object\n",
       "claps             int64\n",
       "responses       float64\n",
       "reading_time    float64\n",
       "publication      object\n",
       "date             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url :  0\n",
      "title :  0\n",
      "subtitle :  3027\n",
      "image :  146\n",
      "claps :  0\n",
      "responses :  0\n",
      "reading_time :  0\n",
      "publication :  0\n",
      "date :  0\n"
     ]
    }
   ],
   "source": [
    "# Checking for Missing Values\n",
    "for column in range(len(dat.columns)):\n",
    "    print(dat.columns[column], ': ',len(dat[dat.iloc[:,column].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.,  9.,  5.,  4., 12., 18.,  6., 21., 14., 10.,  3., 19.,  7.,\n",
       "       16.,  2., 22., 11., 13., 20., 15.,  1., 40., 32., 17., 27., 31.,\n",
       "       26.,  0., 24., 25., 23., 33., 55., 36.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.iloc[:,6].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">responses</th>\n",
       "      <th colspan=\"2\" halign=\"left\">reading_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Better Humans</th>\n",
       "      <td>28</td>\n",
       "      <td>8.535714</td>\n",
       "      <td>28</td>\n",
       "      <td>13.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Better Marketing</th>\n",
       "      <td>242</td>\n",
       "      <td>4.619835</td>\n",
       "      <td>242</td>\n",
       "      <td>6.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Driven Investor</th>\n",
       "      <td>777</td>\n",
       "      <td>0.373230</td>\n",
       "      <td>777</td>\n",
       "      <td>5.222651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Startup</th>\n",
       "      <td>3041</td>\n",
       "      <td>1.791845</td>\n",
       "      <td>3041</td>\n",
       "      <td>5.906281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Writing Cooperative</th>\n",
       "      <td>403</td>\n",
       "      <td>3.163772</td>\n",
       "      <td>403</td>\n",
       "      <td>4.965261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Towards Data Science</th>\n",
       "      <td>1461</td>\n",
       "      <td>1.732375</td>\n",
       "      <td>1461</td>\n",
       "      <td>7.276523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UX Collective</th>\n",
       "      <td>554</td>\n",
       "      <td>1.471119</td>\n",
       "      <td>554</td>\n",
       "      <td>6.032491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        responses           reading_time           \n",
       "                            count      mean        count       mean\n",
       "publication                                                        \n",
       "Better Humans                  28  8.535714           28  13.357143\n",
       "Better Marketing              242  4.619835          242   6.409091\n",
       "Data Driven Investor          777  0.373230          777   5.222651\n",
       "The Startup                  3041  1.791845         3041   5.906281\n",
       "The Writing Cooperative       403  3.163772          403   4.965261\n",
       "Towards Data Science         1461  1.732375         1461   7.276523\n",
       "UX Collective                 554  1.471119          554   6.032491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking into article count provided by differences sub-publication of Medium\n",
    "dat.groupby('publication').agg({'responses': ['count','mean'],\n",
    "                                'reading_time':['count','mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>https://medium.com/better-marketing/we-vs-i-ho...</td>\n",
       "      <td>“We” vs “I” — How Should You Talk About Yourse...</td>\n",
       "      <td>Basic copywriting choices with a big…</td>\n",
       "      <td>6504.jpg</td>\n",
       "      <td>661</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>https://medium.com/better-marketing/how-donald...</td>\n",
       "      <td>How Donald Trump Markets Himself</td>\n",
       "      <td>Lessons from who might be the most popular bra...</td>\n",
       "      <td>6505.jpeg</td>\n",
       "      <td>189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>https://medium.com/better-marketing/content-an...</td>\n",
       "      <td>Content and Marketing Beyond Mass Consumption</td>\n",
       "      <td>How to acquire customers without wasting money...</td>\n",
       "      <td>6506.jpg</td>\n",
       "      <td>207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>https://medium.com/better-marketing/5-question...</td>\n",
       "      <td>5 Questions All Copywriters Should Ask Clients...</td>\n",
       "      <td>Save time and effort by…</td>\n",
       "      <td>6507.jpg</td>\n",
       "      <td>253</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>https://medium.com/better-marketing/how-to-wri...</td>\n",
       "      <td>How To Write a Good Business Blog Post</td>\n",
       "      <td>An A-to-Z guide for non-writers</td>\n",
       "      <td>6508.jpg</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6506 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0     https://towardsdatascience.com/a-beginners-gui...   \n",
       "1     https://towardsdatascience.com/hands-on-graph-...   \n",
       "2     https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3     https://towardsdatascience.com/databricks-how-...   \n",
       "4     https://towardsdatascience.com/a-step-by-step-...   \n",
       "...                                                 ...   \n",
       "6503  https://medium.com/better-marketing/we-vs-i-ho...   \n",
       "6504  https://medium.com/better-marketing/how-donald...   \n",
       "6505  https://medium.com/better-marketing/content-an...   \n",
       "6506  https://medium.com/better-marketing/5-question...   \n",
       "6507  https://medium.com/better-marketing/how-to-wri...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                          How to Use ggplot2 in Python   \n",
       "3     Databricks: How to Save Files in CSV on Your L...   \n",
       "4     A Step-by-Step Implementation of Gradient Desc...   \n",
       "...                                                 ...   \n",
       "6503  “We” vs “I” — How Should You Talk About Yourse...   \n",
       "6504                   How Donald Trump Markets Himself   \n",
       "6505      Content and Marketing Beyond Mass Consumption   \n",
       "6506  5 Questions All Copywriters Should Ask Clients...   \n",
       "6507             How To Write a Good Business Blog Post   \n",
       "\n",
       "                                               subtitle      image  claps  \\\n",
       "0                                                   NaN      1.png    850   \n",
       "1                                                   NaN      2.png   1100   \n",
       "2                      A Grammar of Graphics for Python      3.png    767   \n",
       "3               When I work on Python projects dealing…     4.jpeg    354   \n",
       "4                       One example of building neural…     5.jpeg    211   \n",
       "...                                                 ...        ...    ...   \n",
       "6503              Basic copywriting choices with a big…   6504.jpg    661   \n",
       "6504  Lessons from who might be the most popular bra...  6505.jpeg    189   \n",
       "6505  How to acquire customers without wasting money...   6506.jpg    207   \n",
       "6506                           Save time and effort by…   6507.jpg    253   \n",
       "6507                    An A-to-Z guide for non-writers   6508.jpg    147   \n",
       "\n",
       "      responses  reading_time           publication        date  \n",
       "0           8.0           8.0  Towards Data Science  2019-05-30  \n",
       "1          11.0           9.0  Towards Data Science  2019-05-30  \n",
       "2           1.0           5.0  Towards Data Science  2019-05-30  \n",
       "3           0.0           4.0  Towards Data Science  2019-05-30  \n",
       "4           3.0           4.0  Towards Data Science  2019-05-30  \n",
       "...         ...           ...                   ...         ...  \n",
       "6503        6.0           6.0      Better Marketing  2019-12-05  \n",
       "6504        1.0           5.0      Better Marketing  2019-12-05  \n",
       "6505        1.0           8.0      Better Marketing  2019-12-05  \n",
       "6506        2.0           5.0      Better Marketing  2019-12-05  \n",
       "6507        0.0           9.0      Better Marketing  2019-12-05  \n",
       "\n",
       "[6506 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.loc[0,'url']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "req = Request(url=dat.loc[0,'url'], headers = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get startedOpen in appSign inGet startedFollow535K Followers·Editors' PicksFeaturesExploreContributeAboutGet startedOpen in appA Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vecMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data ScienceAboutHelpLegalGet the Medium app\n",
      "Get startedOpen in appSign inGet startedFollow535K Followers·Editors' PicksFeaturesExploreContributeAboutGet startedOpen in appA Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vecMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data ScienceAboutHelpLegalGet the Medium app\n",
      "\n",
      "\n",
      "Get startedOpen in appSign inGet startedFollow535K Followers·Editors' PicksFeaturesExploreContributeAboutGet startedOpen in appA Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vecMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data ScienceAboutHelpLegalGet the Medium app\n",
      "Get startedOpen in appSign inGet startedFollow535K Followers·Editors' PicksFeaturesExploreContributeAboutGet startedOpen in app\n",
      "Get startedOpen in appSign inGet startedFollow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Get startedOpen in appSign inGet started\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get started\n",
      "Get started\n",
      "Open in app\n",
      "Open in app\n",
      "Sign inGet started\n",
      "Sign inGet started\n",
      "Sign inGet started\n",
      "Sign inGet started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sign inGet started\n",
      "Sign in\n",
      "Get started\n",
      "Get started\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow535K Followers·Editors' PicksFeaturesExploreContributeAbout\n",
      "Follow\n",
      "Follow\n",
      "Follow\n",
      "Follow\n",
      "535K Followers\n",
      "·\n",
      "Editors' PicksFeaturesExploreContribute\n",
      "About\n",
      "\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get startedOpen in app\n",
      "Get started\n",
      "Open in app\n",
      "Open in app\n",
      "\n",
      "A Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.\n",
      "\n",
      "A Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations\n",
      "A Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min readWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for the vehicle make model.Table of Contents1. Introduction of Word2vec2. Gensim Python Library Introduction3. Implementation of word Embedding with Gensim Word2Vec Model3.1 Data Preprocessing:3.2. Genism word2vec Model Training4. Compute Similarities5. T-SNE Visualizations\n",
      "A Beginner’s Guide to Word Embedding with Gensim Word2Vec ModelZhi LiMay 30, 2019·8 min read\n",
      "Zhi LiMay 30, 2019·8 min read\n",
      "Zhi LiMay 30, 2019·8 min read\n",
      "Zhi LiMay 30, 2019·8 min read\n",
      "\n",
      "Zhi LiMay 30, 2019·8 min read\n",
      "Zhi Li\n",
      "Zhi Li\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.\n",
      "1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.2. Gensim Python Library IntroductionGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)NumPy >= 1.11.3SciPy >= 0.18.1Six >= 1.5.0smart_open >= 1.2.1There are two ways for installation. We could run the following code in our terminal to install genism package.pip install --upgrade gensimOr, alternatively for Conda environments:conda install -c conda-forge gensim3. Implementation of word Embedding with Gensim Word2Vec ModelIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.>>> df = pd.read_csv('data.csv')>>> df.head()3.1 Data Preprocessing:Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.To achieve this, we need to do the following things :a. Create a new column for Make Model>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.# Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']]# For each row, combine all the columns into one column>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe>>> df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']]# show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2][['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Factory Tuner',  'Luxury',  'High-Performance',  'Compact',  'Coupe',  'BMW 1 Series M'], ['premium unleaded (required)',  'MANUAL',  'rear wheel drive',  'Luxury',  'Performance',  'Compact',  'Convertible',  'BMW 1 Series']]3.2. Genism word2vec Model TrainingWe can train the genism word2vec model with our own custom corpus as following:>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)Let’s try to understand the hyperparameters of this model.size: The number of dimensions of the embeddings and the default is 100.window: The maximum distance between a target word and words around the target word. The default window is 5.min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.workers: The number of partitions during training and the default workers is 3.sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.After training the word2vec model, we can obtain the word embedding directly from the training model as following.>>> model['Toyota Camry']array([-0.11884457,  0.03035539, -0.0248678 , -0.06297892, -0.01703234,       -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,       -0.07199778,  0.05235871,  0.21303181,  0.15767808, -0.1883737 ,        0.01938575, -0.24431638,  0.04261152,  0.11865819,  0.09881561,       -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,       -0.0040625 ,  0.16796461,  0.14578669,  0.04187112, -0.01436194,       -0.25554284,  0.25494182,  0.05522631,  0.19295982,  0.14461821,        0.14022525, -0.2065216 , -0.05020927, -0.08133671,  0.18031682,        0.35042757,  0.0245426 ,  0.15938364, -0.05617865,  0.00297452,        0.15442047, -0.01286271,  0.13923576,  0.085941  ,  0.18811756],      dtype=float32)4. Compute SimilaritiesNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')0.822824584626184>>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')0.961089779453727From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5][('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)]However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.The following function shows how can we generate the most similar make model based on cosine similarity.def cosine_distance (model, word,target_list , num) :    cosine_dict ={}    word_list = []    a = model[word]    for item in target_list :        if item != word :            b = model [item]            cos_sim = dot(a, b)/(norm(a)*norm(b))            cosine_dict[item] = cos_sim    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order     for item in dist_sort:        word_list.append((item[0], item[1]))    return word_list[0:num]# only get the unique Maker_Model>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5)[('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)]5. T-SNE VisualizationsIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.def display_closestwords_tsnescatterplot(model, word, size):        arr = np.empty((0,size), dtype='f')    word_labels = [word]close_words = model.similar_by_word(word)arr = np.append(arr, np.array([model[word]]), axis=0)    for wrd_score in close_words:        wrd_vector = model[wrd_score[0]]        word_labels.append(wrd_score[0])        arr = np.append(arr, np.array([wrd_vector]), axis=0)            tsne = TSNE(n_components=2, random_state=0)    np.set_printoptions(suppress=True)    Y = tsne.fit_transform(arr)x_coords = Y[:, 0]    y_coords = Y[:, 1]    plt.scatter(x_coords, y_coords)for label, x, y in zip(word_labels, x_coords, y_coords):        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)    plt.show()>>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.About MeI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow1.2K 7 \n",
      "Written byZhi LiMaster student in Data Science at University of San Francisco.Follow\n",
      "Zhi Li\n",
      "Master student in Data Science at University of San Francisco.\n",
      "Follow\n",
      "1.2K \n",
      "1.2K \n",
      "\n",
      "\n",
      "1.2K \n",
      "1.2K \n",
      "7 \n",
      "7 \n",
      "7 \n",
      "\n",
      "\n",
      "Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vecMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vecMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vec\n",
      "Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.1.2K 1.2K 7 Machine LearningNLPWord EmbeddingsGensimWord2vec\n",
      "Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.\n",
      "Sign up for The Daily Pick\n",
      "By Towards Data Science\n",
      "Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look\n",
      "Get this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.\n",
      "Get this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.\n",
      "\n",
      "Get this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.\n",
      "Get this newsletter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get this newsletter\n",
      "Get this newsletter\n",
      "By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.\n",
      "Check your inboxMedium sent you an email at  to complete your subscription.\n",
      "\n",
      "\n",
      "1.2K 1.2K 7 \n",
      "1.2K 1.2K 7 \n",
      "1.2K 1.2K 7 \n",
      "1.2K 1.2K \n",
      "1.2K \n",
      "\n",
      "\n",
      "1.2K \n",
      "1.2K \n",
      "1.2K \n",
      "\n",
      "\n",
      "1.2K \n",
      "1.2K \n",
      "\n",
      "7 \n",
      "\n",
      "\n",
      "7 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Machine LearningNLPWord EmbeddingsGensimWord2vec\n",
      "More from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "More from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.\n",
      "More from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.\n",
      "More from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.\n",
      "More from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.\n",
      "More from Towards Data ScienceFollow\n",
      "Follow\n",
      "Follow\n",
      "A Medium publication sharing concepts, ideas, and codes.\n",
      "Read more from Towards Data Science\n",
      "Read more from Towards Data Science\n",
      "Read more from Towards Data Science\n",
      "Read more from Towards Data Science\n",
      "Read more from Towards Data Science\n",
      "More From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "More From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "More From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "More From MediumStop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "More From Medium\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data ScienceHow to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data ScienceThe Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data ScienceThree Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data ScienceSocial Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data ScienceIs Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream InsteadKhuyen Tran in Towards Data Science\n",
      "Stop Using Print to Debug in Python. Use Icecream Instead\n",
      "Khuyen Tran in Towards Data Science\n",
      "\n",
      "Khuyen Tran in Towards Data Science\n",
      "Khuyen Tran in Towards Data Science\n",
      "Khuyen Tran in Towards Data Science\n",
      "Khuyen Tran in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science\n",
      "7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science\n",
      "7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science\n",
      "7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science\n",
      "7 A/B Testing Questions and Answers in Data Science InterviewsEmma Ding in Towards Data Science\n",
      "7 A/B Testing Questions and Answers in Data Science Interviews\n",
      "Emma Ding in Towards Data Science\n",
      "\n",
      "Emma Ding in Towards Data Science\n",
      "Emma Ding in Towards Data Science\n",
      "Emma Ding in Towards Data Science\n",
      "Emma Ding in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data Science\n",
      "10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data Science\n",
      "10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data Science\n",
      "10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data Science\n",
      "10 Surprisingly Useful Base Python FunctionsEmmett Boudreau in Towards Data Science\n",
      "10 Surprisingly Useful Base Python Functions\n",
      "Emmett Boudreau in Towards Data Science\n",
      "\n",
      "Emmett Boudreau in Towards Data Science\n",
      "Emmett Boudreau in Towards Data Science\n",
      "Emmett Boudreau in Towards Data Science\n",
      "Emmett Boudreau in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data Science\n",
      "How to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data Science\n",
      "How to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data Science\n",
      "How to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data Science\n",
      "How to Become a Data Analyst and a Data ScientistVicky Yu in Towards Data Science\n",
      "How to Become a Data Analyst and a Data Scientist\n",
      "Vicky Yu in Towards Data Science\n",
      "\n",
      "Vicky Yu in Towards Data Science\n",
      "Vicky Yu in Towards Data Science\n",
      "Vicky Yu in Towards Data Science\n",
      "Vicky Yu in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data Science\n",
      "The Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data Science\n",
      "The Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data Science\n",
      "The Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data Science\n",
      "The Best Data Science Project to Have in Your PortfolioSoner Yıldırım in Towards Data Science\n",
      "The Best Data Science Project to Have in Your Portfolio\n",
      "Soner Yıldırım in Towards Data Science\n",
      "\n",
      "Soner Yıldırım in Towards Data Science\n",
      "Soner Yıldırım in Towards Data Science\n",
      "Soner Yıldırım in Towards Data Science\n",
      "Soner Yıldırım in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Three Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data Science\n",
      "Three Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data Science\n",
      "Three Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data Science\n",
      "Three Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data Science\n",
      "Three Concepts to Become a Better Python ProgrammerLuay Matalka in Towards Data Science\n",
      "Three Concepts to Become a Better Python Programmer\n",
      "Luay Matalka in Towards Data Science\n",
      "\n",
      "Luay Matalka in Towards Data Science\n",
      "Luay Matalka in Towards Data Science\n",
      "Luay Matalka in Towards Data Science\n",
      "Luay Matalka in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data Science\n",
      "Social Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data Science\n",
      "Social Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data Science\n",
      "Social Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data Science\n",
      "Social Network Analysis: From Graph Theory to Applications with PythonDima Goldenberg in Towards Data Science\n",
      "Social Network Analysis: From Graph Theory to Applications with Python\n",
      "Dima Goldenberg in Towards Data Science\n",
      "\n",
      "Dima Goldenberg in Towards Data Science\n",
      "Dima Goldenberg in Towards Data Science\n",
      "Dima Goldenberg in Towards Data Science\n",
      "Dima Goldenberg in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?Anna Anisienia in Towards Data Science\n",
      "Is Apache Airflow 2.0 good enough for current data engineering needs?\n",
      "Anna Anisienia in Towards Data Science\n",
      "\n",
      "Anna Anisienia in Towards Data Science\n",
      "Anna Anisienia in Towards Data Science\n",
      "Anna Anisienia in Towards Data Science\n",
      "Anna Anisienia in Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AboutHelpLegalGet the Medium app\n",
      "AboutHelpLegalGet the Medium app\n",
      "AboutHelpLegalGet the Medium app\n",
      "AboutHelpLegalGet the Medium app\n",
      "AboutHelpLegal\n",
      "AboutHelpLegal\n",
      "Get the Medium app\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req =  Request(dat.loc[0,'url'], headers = header)\n",
    "\n",
    "soup = BeautifulSoup(urlopen(req), 'html.parser')\n",
    "for link in soup.find_all('div'): # It helps to find all anchor tag's\n",
    "    print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
